{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c914d050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Prophesee Dataset Toolbox could not be found!\n",
      "         Only Prophesee DVS demo will not run properly.\n",
      "         Please install it from https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "import lava.lib.dl.bootstrap as bootstrap\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa637614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, time_steps=16):\n",
    "        super(Network, self).__init__()\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 1, # this must be 1 to use batchnorm\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 1,\n",
    "                'scale_grad'    : 1,\n",
    "            }\n",
    "        neuron_params_norm = {\n",
    "                **neuron_params, \n",
    "                # 'norm'    : slayer.neuron.norm.MeanOnlyBatchNorm,\n",
    "            }\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                bootstrap.block.cuba.Input(neuron_params, weight=1, bias=0), # enable affine transform at input\n",
    "                bootstrap.block.cuba.Dense(neuron_params_norm, 28*28, 512, weight_norm=True, weight_scale=2),\n",
    "                bootstrap.block.cuba.Dense(neuron_params_norm, 512, 512, weight_norm=True, weight_scale=2),\n",
    "                bootstrap.block.cuba.Affine(neuron_params, 512, 10, weight_norm=True, weight_scale=2),\n",
    "            ])\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        N, C, H, W = x.shape\n",
    "        if mode.base_mode == bootstrap.Mode.ANN:\n",
    "            x = x.reshape([N, C, H, W, 1])\n",
    "        else:\n",
    "            x = slayer.utils.time.replicate(x, self.time_steps)\n",
    "\n",
    "        x = x.reshape(N, -1, x.shape[-1])\n",
    "\n",
    "        for block, m in zip(self.blocks, mode):\n",
    "            x = block(x, mode=m)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        simulation = h.create_group('simulation')\n",
    "        simulation['Ts'] = 1\n",
    "        simulation['tSample'] = self.time_steps        \n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d8d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xsb24130\\AppData\\Local\\miniconda3\\envs\\loihi-env\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3745490.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 394446.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2149543.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') \n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Dataset and dataLoader instances.\n",
    "training_set = datasets.MNIST(\n",
    "        root='data/',\n",
    "        train=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.RandomAffine(\n",
    "                degrees=10, \n",
    "                translate=(0.05, 0.05),\n",
    "                scale=(0.95, 1.05),\n",
    "                shear=5,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5)),\n",
    "        ]),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "testing_set = datasets.MNIST(\n",
    "        root='data/',\n",
    "        train=False,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5)),\n",
    "        ]),\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(dataset=training_set, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=testing_set , batch_size=32, shuffle=True)\n",
    "\n",
    "stats = slayer.utils.LearningStats()\n",
    "scheduler = bootstrap.routine.Scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ace337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([32, 1, 28, 28]), Label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "datas, labels = next(iter(train_loader))\n",
    "print(f\"Data shape: {datas.shape}, Label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5aaac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loihi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
